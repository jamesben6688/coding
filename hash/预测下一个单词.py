import re
from collections import defaultdict

"""
trainä¸€ä¸ªlist of sentencesæ¥é¢„æµ‹ä¸€ä¸ªstringåé¢æœ€æœ‰å¯èƒ½å‡ºç°å“ªä¸ªå•è¯ã€‚
ç”¨äº†ä¸¤ä¸ªmap followupäº†æ—¶é—´å¤æ‚åº¦ å¦‚ä½•ä¼˜åŒ– æˆ‘çš„solutionæœ¬æ¥æ˜¯O(n) é—®äº†å¦‚ä½•ä¼˜åŒ–æˆ‘è¯´é‚£å°±åº”è¯¥æœ‰O1çš„solutionå’¯ 
å°å“¥ç¬‘äº†é—®æˆ‘æ€ä¹ˆå®ç° æ”¹äº†ä¸€ä¸‹map åˆè¢«é—®äº†å¦‚æœä¸€ä¸ªå•è¯æœ‰1/3å¯èƒ½æ€§å‡ºç°å¦å¤–ä¸€ä¸ª2/3å¯èƒ½æ€§å‡ºç° å¦‚ä½•ä¿è¯å‡ºç°æ¦‚ç‡

followup:
 å¦‚æœä½ æƒ³è®©é¢„æµ‹ç»“æœç¬¦åˆè®­ç»ƒä¸­å­¦åˆ°çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆæ¯”å¦‚æŸä¸ªå•è¯åé¢æœ‰ 1/3 çš„æ¦‚ç‡æ˜¯ "apple"ï¼Œ2/3 çš„æ¦‚ç‡æ˜¯ "banana"ï¼‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±è¦åœ¨é¢„æµ‹é˜¶æ®µç”¨ éšæœºé‡‡æ ·ï¼ˆsamplingï¼‰ï¼Œè€Œä¸æ˜¯æ€»æ˜¯å–æœ€å¤§å€¼ï¼ˆargmaxï¼‰ã€‚

ğŸ§  æ ¸å¿ƒåŒºåˆ«
argmaxï¼šæ€»æ˜¯è¿”å›æ¦‚ç‡æœ€å¤§çš„é‚£ä¸ªè¯ â†’ ç¡®å®šæ€§ï¼ˆä¸ä¿ç•™æ¦‚ç‡ä¿¡æ¯ï¼‰ã€‚

samplingï¼šæ ¹æ®æ¦‚ç‡åˆ†å¸ƒè¿›è¡ŒæŠ½æ · â†’ ç¬¦åˆç»Ÿè®¡è§„å¾‹ï¼ˆå¯ä»¥è¿˜åŸè®­ç»ƒåˆ†å¸ƒï¼‰ã€‚
random.choices(words, weights=weights, k=1)
"""


class BigramPredictor:
    def __init__(self):
        self.model = defaultdict(Counter)

    def tokenize(self, sentence):
        return re.findall(r'\b\w+\b', sentence.lower())

    def train(self, sentences):
        for sentence in sentences:
            tokens = self.tokenize(sentence)
            for i in range(len(tokens) - 1):
                self.model[tokens[i]][tokens[i + 1]] += 1

    def predict_next(self, input_string, topk=1):
        tokens = self.tokenize(input_string)
        if not tokens:
            return None
        last_word = tokens[-1]
        next_words = self.model.get(last_word)
        if not next_words:
            return None
        return [word for word, _ in next_words.most_common(topk)]

    def sample_next(self, input_string):
        tokens = self.tokenize(input_string)
        if not tokens:
            return None
        last_word = tokens[-1]
        counter = self.model.get(last_word)
        if not counter:
            return None
        words, weights = zip(*counter.items())
        return random.choices(words, weights=weights, k=1)[0]
